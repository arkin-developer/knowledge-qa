"""é˜…è¯»æœ¬åœ°æ–‡ä»¶èµ„æ–™çš„å·¥å…·æ™ºèƒ½ä½“"""

import os 
import json
import re
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain import hub
from langchain_core.tools import tool, Tool
from langchain_core.prompts import PromptTemplate
from langchain_core.callbacks import BaseCallbackHandler
from pydantic import BaseModel, Field
from ..config import settings
from ..log_manager import log


class DocumentFragmentMeta(BaseModel):
    """æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®ï¼ˆä¸å«å†…å®¹ï¼ŒèŠ‚çœtokenï¼‰"""
    filename: str
    start_line: int
    end_line: int

class DocumentFragment(BaseModel):
    """æ–‡æ¡£ç‰‡æ®µ"""
    filename: str
    content: str
    start_line: int
    end_line: int

class SearchKeywordToolInput(BaseModel):
    keyword: str = Field(..., description="å…³é”®è¯")
    filename: str = Field(..., description="æ–‡ä»¶å")
    limit: int = Field(default=30, description="æœ€å¤§è¿”å›ç»“æœæ•°")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"keyword": "å…³é”®è¯", "filename": "æ–‡ä»¶å", "limit": 100}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "keyword": "å…³é”®è¯ (å¿…å¡«)",
            "filename": "æ–‡ä»¶å (å¿…å¡«)", 
            "limit": "æœ€å¤§è¿”å›ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤100)"
        }

class ReadFileContentToolInput(BaseModel):
    filename: str = Field(..., description="æ–‡ä»¶å")
    start_line: int = Field(..., description="èµ·å§‹è¡Œå·")
    end_line: int = Field(..., description="ç»“æŸè¡Œå·")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"filename": "æ–‡ä»¶å", "start_line": 1, "end_line": 10}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "filename": "æ–‡ä»¶å (å¿…å¡«)",
            "start_line": "èµ·å§‹è¡Œå· (å¿…å¡«)",
            "end_line": "ç»“æŸè¡Œå· (å¿…å¡«)"
        }

class AddFragmentMetaToolInput(BaseModel):
    filename: str = Field(..., description="æ–‡ä»¶å")
    start_line: int = Field(..., description="èµ·å§‹è¡Œå·")
    end_line: int = Field(..., description="ç»“æŸè¡Œå·")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"filename": "æ–‡ä»¶å", "start_line": 1, "end_line": 10}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "filename": "æ–‡ä»¶å (å¿…å¡«)",
            "start_line": "èµ·å§‹è¡Œå· (å¿…å¡«)",
            "end_line": "ç»“æŸè¡Œå· (å¿…å¡«)"
        }

class ReaderLLM:
    """é˜…è¯»æœ¬åœ°æ–‡ä»¶èµ„æ–™çš„å·¥å…·æ™ºèƒ½ä½“"""

    def __init__(self):
        self.upload_path = settings.upload_temp_path
        self.tools = self._create_all_tools()
        self.llm = ChatOpenAI(
            openai_api_key=settings.siliconcloud_api_key,
            openai_api_base=settings.siliconcloud_api_base,
            model=settings.llm_model,
            temperature=settings.llm_temperature
        )

        # æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨
        self.fragments_meta = []
        # å®Œæ•´æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ˆç¼–ç¨‹è·å–ï¼Œä¿è¯ç²¾å‡†åŒ¹é…ï¼‰
        self.fragments = []
        
        # åˆ›å»ºæ—¥å¿—è®°å½•å›è°ƒï¼ˆè°ƒè¯•ï¼‰
        self.agent_callback = AgentLoggingCallback()
        
        for tool in self.tools:
            log.info(f"  - {tool.name}: {tool.description}")

        self.agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self._get_prompt_template()
        )
        
        # åˆ›å»ºAgentExecutoræ¥æ‰§è¡Œagent
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=False,
            handle_parsing_errors=True,
            callbacks=[self.agent_callback],
            max_iterations=10,
            early_stopping_method="force"
        )
        log.info("Agent å’Œ AgentExecutor åˆ›å»ºå®Œæˆ")
    
    def _get_prompt_template(self) -> PromptTemplate:
        """åˆ›å»ºReAct Agentçš„promptæ¨¡æ¿ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼‰,å®˜æ–¹å»ºè®®react agent çš„prompt ç”¨è‹±æ–‡"""
        return PromptTemplate.from_template("""
Answer the following questions as best you can. You have access to the following tools:

{tools}

**Tool Usage Instructions:**
- list_files_tool_func(): List all available files, no parameters needed
- search_keyword_tool_func(keyword, filename, limit=300): Search for keywords in files and return relevant lines with context
  Purpose: Find specific content related to keywords, returns line numbers and surrounding context
  Parameters: keyword (search term - can be multiple keywords), filename (file name), limit (max results, default 300)
  Use this when: You need to find specific information or content within a file
  
  **KEYWORD SEPARATION RULES:**
  - **Chinese keywords**: Use spaces to separate multiple keywords
    Examples: "éŸ©ç«‹ ä¸ƒç„é—¨", "æ³•æœ¯ ä¿®ç‚¼", "å¢¨å¤§å¤« æ¥å†", "æ­»å¥‘ è¡€æ–—"
  - **English keywords**: Use COMMAS to separate multiple keywords (preserves phrases)
    Examples: "bonus action, spellcasting", "casting time, cantrip", "Grappled, status"
    Examples: "bonus action spell", "spellcasting rules" (single phrases)
  
  The function will find lines containing ANY of these keywords (OR logic)
- read_file_content_tool_func(filename, start_line, end_line): Read file content by line range
  Purpose: Get detailed content from specific line ranges, useful after finding relevant lines with search
  Parameters: filename (file name), start_line (start line number), end_line (end line number)
  Use this when: You need to read more detailed content after finding relevant lines with search_keyword_tool_func
- add_fragment_meta_tool_func(fragments): Add document fragment metadata to the system
  Purpose: Store relevant document fragments for future reference and context building
  Parameters: fragments (array of fragment objects with filename, start_line, end_line)
  Use this when: You find relevant content that should be saved for the user's question
  Note: You can call this multiple times and can input single or multiple fragments as an array
  **CRITICAL**: This is your PRIMARY GOAL - save ALL relevant fragments you discover
  Format examples:
    Single fragment: {{"filename": "file.txt", "start_line": 10, "end_line": 20}}
    Multiple fragments: {{"fragments": [{{"filename": "file.txt", "start_line": 10, "end_line": 20}}, {{"filename": "file.txt", "start_line": 30, "end_line": 40}}]}}
    
  **CRITICAL JSON FORMAT REQUIREMENTS**:
  - Use standard JSON format with proper punctuation
  - Numbers (start_line, end_line) must NOT have quotes around them
  - Use English punctuation only (no Chinese punctuation)
  - Use single-line JSON format to avoid parsing errors
  - Example: {{"filename": "file.txt", "start_line": 10, "end_line": 20}} âœ…
  - Wrong: {{"filename": "file.txt", "start_line": "10", "end_line": "20"}} âŒ (numbers should not have quotes)
  - Wrong: {{"filename": "Player's Handbook.md", "start_line": 16845, "end_line": 16852"}} âŒ (extra quote after number)

**Workflow Guidelines:**
1. First, use list_files_tool_func() to see available files
2. Use search_keyword_tool_func() to find relevant content (returns lines containing keywords)
3. **EVALUATE CONTENT**: After finding relevant lines, check if the keyword-containing lines provide enough context:
   - **If context is sufficient**: Directly use add_fragment_meta_tool_func() to save the relevant fragments
   - **If context is insufficient**: Use read_file_content_tool_func() to get surrounding lines for more context
4. **CRITICAL**: Use add_fragment_meta_tool_func() to save ALL relevant fragments you find (this is your PRIMARY GOAL)
5. Always provide a complete answer based on the information you gather

**Key Understanding**: 
- search_keyword_tool_func() returns **complete lines** that contain the keywords
- read_file_content_tool_func() returns **surrounding context** around specific line ranges
- Only use read_file_content_tool_func() when the keyword-containing lines lack sufficient context to answer the question fully

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action (use the correct parameter names as shown above)
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

**Important Notes:**
- **YOUR MAIN OBJECTIVE**: Find and save ALL relevant document fragments using add_fragment_meta_tool_func
- **SMART CONTEXT STRATEGY**: After finding relevant content with search_keyword_tool_func:
  * **Understand the difference**: search_keyword_tool_func() returns complete lines containing keywords, read_file_content_tool_func() returns surrounding context
  * **Evaluate first**: Check if the keyword-containing lines provide enough context to answer the question
  * **If sufficient**: Save the fragments directly using add_fragment_meta_tool_func()
  * **If insufficient**: Use read_file_content_tool_func() to get surrounding lines for more context
  * **Be strategic**: Only read file content when the keyword-containing lines lack sufficient context
- **SEARCH STRATEGY**:
  * **English keywords**: Use commas to separate multiple keywords (e.g., "bonus action, spellcasting", "casting time, cantrip")
  * **Chinese keywords**: Use spaces to separate multiple keywords (e.g., "éŸ©ç«‹ ä¸ƒç„é—¨", "æ³•æœ¯ ä¿®ç‚¼")
  * **Mixed language**: Use appropriate separator for each language (e.g., "bonus action, å¥–åŠ±åŠ¨ä½œ")
  * **Single phrases**: Keep as one unit (e.g., "bonus action spell", "spellcasting rules")
- **MANDATORY**: Use add_fragment_meta_tool_func to save important fragments you discover (supports both single and multiple fragments)
- You can call add_fragment_meta_tool_func multiple times throughout your search process
- **PRIORITY**: Saving relevant fragments is more important than just answering the question
- Always follow the exact format: Thought -> Action -> Action Input -> Observation
- If you have enough information to answer the question, proceed to Final Answer
- Never skip the Action line - always include it when you want to use a tool

Begin!

Question: {input}
Thought:{agent_scratchpad}
        """)
    
    def generate(self, query: str) -> str:
        """æ ¹æ®é—®é¢˜ç”Ÿæˆå›ç­”"""
        log.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        try:
            result = self.agent_executor.invoke({"input": query})
            log.info("æŸ¥è¯¢å¤„ç†å®Œæˆ")
            return result
        except Exception as e:
            log.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            log.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            log.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            log.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise

    def _create_all_tools(self) -> List[Tool]:
        """åˆ›å»ºæ‰€æœ‰å·¥å…·"""
        tools = [
            self._list_files_tool(),
            self._search_keyword_tool(),
            self._read_file_content_tool(),
            self._add_fragment_meta_tool()
        ]
        return tools

    def _list_files_tool(self):
        """åˆ—å‡ºæ–‡ä»¶å·¥å…·"""
        @tool
        def list_files_tool_func() -> str:
            """åˆ—å‡ºä¸Šä¼ æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
            try:
                log.info(f"æ­£åœ¨æ£€æŸ¥ç›®å½•: {self.upload_path}")
                if not os.path.exists(self.upload_path):
                    log.warning(f"ç›®å½•ä¸å­˜åœ¨: {self.upload_path}")
                    return f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.upload_path}"

                files = []
                for filename in os.listdir(self.upload_path):
                    file_path = os.path.join(self.upload_path, filename)
                    if os.path.isfile(file_path):
                        files.append(filename)
                
                result = json.dumps({"files": files}, ensure_ascii=False, indent=2)
                log.info(f"æ‰¾åˆ°æ–‡ä»¶: {result}")
                return result
            except Exception as e:
                log.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                return f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}"
        return list_files_tool_func

    def _search_keyword_tool(self):
        """æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹å·¥å…·"""
        @tool
        def search_keyword_tool_func(input: SearchKeywordToolInput | str) -> str:
            """åœ¨æŒ‡å®šæ–‡ä»¶ä¸­æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹"""
            # å‚æ•°è§£æ
            try:
                if isinstance(input, str):
                    input = json.loads(input)
                    keyword = input.get("keyword")
                    filename = input.get("filename")
                    limit = input.get("limit")
                else:
                    keyword = input.keyword
                    filename = input.filename
                    limit = input.limit
            except Exception as e:
                log.error(f"æœç´¢å…³é”®è¯å¤±è´¥: {e}")
                return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {SearchKeywordToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

            try:
                # æ™ºèƒ½å…³é”®è¯åˆ†å‰²ï¼šæ ¹æ®è¯­è¨€ç‰¹ç‚¹é€‰æ‹©åˆ†å‰²æ–¹å¼
                keywords = self._split_keywords(keyword)

                file_path = os.path.join(self.upload_path, filename)
                if not os.path.exists(file_path):
                    return f"{filename}æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ ¸å®æ–‡ä»¶åç§°æ˜¯å¦æ­£ç¡®ã€‚"
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                        break
                    except UnicodeDecodeError:
                        return f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ï¼Œç¼–ç ä¸æ”¯æŒ"

                lines = content.split('\n')
                relevant_lines = []
                for i, line in enumerate(lines, 1):
                    if any(keyword.lower() in line.lower() for keyword in keywords):
                        # æ‰©å¤§ä¸Šä¸‹æ–‡èŒƒå›´ï¼Œå¢åŠ åŒ¹é…æ¦‚ç‡
                        # content = lines[i-1].strip() + line.strip() + lines[i+1].strip() 
                        relevant_lines.append({
                            "line_number": i,
                            "content": line.strip()  
                        })
                if limit is None:
                    limit = 30  # é»˜è®¤å€¼
                if limit > 150:
                    limit = 150
                if len(relevant_lines) > limit:
                    relevant_lines = self._smart_sample_lines(relevant_lines, limit)
                result = {
                    "filename": filename,
                    "total_matches": len(relevant_lines),
                    "relevant_lines": relevant_lines
                }
                return json.dumps(result, ensure_ascii=False, indent=2)
                log.info(f"æœç´¢å…³é”®è¯ {keyword} åœ¨æ–‡ä»¶ {filename} ä¸­æ‰¾åˆ° {len(relevant_lines)} è¡Œç›¸å…³å†…å®¹ï¼Œè¿”å›å‰ {len(relevant_lines)} è¡Œ")
            except Exception as e:
                log.error(f"æœç´¢å…³é”®è¯å¤±è´¥: {e}")
                return f"æœç´¢å…³é”®è¯å¤±è´¥: {e}"
        return search_keyword_tool_func

    def _fix_json_format(self, json_str: str) -> str:
        """ä¿®å¤JSONæ ¼å¼é—®é¢˜"""
        # ç§»é™¤å°¾éšé€—å·
        fixed_input = re.sub(r',\s*}', '}', json_str)
        fixed_input = re.sub(r',\s*]', ']', fixed_input)
        
        # ç§»é™¤æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦
        fixed_input = re.sub(r'\n\s*', '', fixed_input)
        fixed_input = re.sub(r'\t', ' ', fixed_input)
        
        # ä¿®å¤ä¸é—­åˆçš„åŒå¼•å·é—®é¢˜
        fixed_input = re.sub(r'(\d+)"\s*}', r'\1}', fixed_input)
        fixed_input = re.sub(r'(\d+)"\s*]', r'\1]', fixed_input)
        fixed_input = re.sub(r'(\d+)"\s*,', r'\1,', fixed_input)
        
        # ä¿®å¤ä¸åŒ¹é…çš„æ‹¬å·
        fixed_input = self._fix_unmatched_brackets(fixed_input)
        
        return fixed_input.strip()
    
    def _fix_unmatched_brackets(self, json_str: str) -> str:
        """ä¿®å¤ä¸åŒ¹é…çš„æ‹¬å·"""
        # ç»Ÿè®¡æ‹¬å·æ•°é‡
        open_braces = json_str.count('{')
        close_braces = json_str.count('}')
        open_brackets = json_str.count('[')
        close_brackets = json_str.count(']')
        
        # ä¿®å¤ç¼ºå¤±çš„é—­åˆæ‹¬å·
        if open_braces > close_braces:
            json_str += '}' * (open_braces - close_braces)
        if open_brackets > close_brackets:
            json_str += ']' * (open_brackets - close_brackets)
        
        return json_str

    def _split_keywords(self, keyword_str: str) -> List[str]:
        """æ™ºèƒ½åˆ†å‰²å…³é”®è¯ï¼Œæ ¹æ®è¯­è¨€ç‰¹ç‚¹é€‰æ‹©åˆ†å‰²æ–¹å¼"""
        if not keyword_str:
            return []
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        has_chinese = bool(re.search(r'[\u4e00-\u9fff]', keyword_str))
        
        if has_chinese:
            # ä¸­æ–‡ï¼šä½¿ç”¨ç©ºæ ¼å’Œé€—å·åˆ†å‰²
            keywords = re.split(r'[,\s]+', keyword_str)
        else:
            # è‹±æ–‡ï¼šä¼˜å…ˆä½¿ç”¨é€—å·åˆ†å‰²ï¼Œå¦‚æœæ²¡æœ‰é€—å·åˆ™ä½¿ç”¨ç©ºæ ¼
            if ',' in keyword_str:
                keywords = [k.strip() for k in keyword_str.split(',') if k.strip()]
            else:
                # å¯¹äºè‹±æ–‡ï¼Œä¿æŒçŸ­è¯­å®Œæ•´æ€§ï¼Œåªåœ¨æ˜æ˜¾åˆ†éš”å¤„åˆ†å‰²
                keywords = re.split(r'\s+', keyword_str)
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        keywords = [k.strip() for k in keywords if k.strip()]
        
        return keywords

    def _smart_sample_lines(self, relevant_lines: List[Dict], limit: int) -> List[Dict]:
        """æ™ºèƒ½é‡‡æ ·ç›¸å…³è¡Œï¼Œä¼˜å…ˆé€‰æ‹©åŒ…å«å¤šä¸ªå…³é”®è¯çš„é‡è¦è¡Œ"""
        if len(relevant_lines) <= limit:
            return relevant_lines
        
        # æŒ‰è¡Œå·æ’åº
        sorted_lines = sorted(relevant_lines, key=lambda x: x['line_number'])
        total_lines = len(sorted_lines)
        
        if total_lines <= limit:
            return sorted_lines
        
        # è®¡ç®—æ¯è¡Œçš„é‡è¦æ€§åˆ†æ•°ï¼ˆåŸºäºå…³é”®è¯å¯†åº¦å’Œå†…å®¹é•¿åº¦ï¼‰
        def calculate_importance(line_info):
            content = line_info['content'].lower()
            # è®¡ç®—å…³é”®è¯å¯†åº¦
            keyword_count = sum(1 for keyword in ['bonus action', 'spellcasting', 'cantrip', 'casting time', 'same turn'] 
                              if keyword in content)
            # è®¡ç®—å†…å®¹é•¿åº¦ï¼ˆé¿å…è¿‡çŸ­çš„è¡Œï¼‰
            content_length = len(content)
            # ç»¼åˆåˆ†æ•°ï¼šå…³é”®è¯å¯†åº¦ + å†…å®¹é•¿åº¦æƒé‡
            return keyword_count * 10 + min(content_length / 10, 5)
        
        # æŒ‰é‡è¦æ€§æ’åº
        scored_lines = [(line, calculate_importance(line)) for line in sorted_lines]
        scored_lines.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©å‰limitä¸ªæœ€é‡è¦çš„è¡Œ
        sampled_lines = [line for line, score in scored_lines[:limit]]
        
        # æŒ‰è¡Œå·é‡æ–°æ’åº
        return sorted(sampled_lines, key=lambda x: x['line_number'])

    def read_file_by_lines(self, input: Dict[str, Any] | str) -> str:
        """è¯»å–æŒ‡å®šæ–‡ä»¶çš„è¡Œå·èŒƒå›´å†…çš„å†…å®¹ï¼Œè¿”å›å­—ç¬¦ä¸²"""
        # å‚æ•°è§£æ
        try:
            if isinstance(input, str):
                # å°è¯•è§£æJSONï¼Œå¤±è´¥åˆ™ä¿®å¤æ ¼å¼åé‡è¯•
                try:
                    parsed_input = json.loads(input.strip())
                except json.JSONDecodeError:
                    # ä¿®å¤JSONæ ¼å¼é—®é¢˜
                    fixed_input = self._fix_json_format(input)
                    parsed_input = json.loads(fixed_input)
                
                filename = parsed_input.get("filename")
                # å…¼å®¹ start_line/end_line å’Œ start_index/end_index
                start_line = parsed_input.get("start_line") or parsed_input.get("start_index")
                end_line = parsed_input.get("end_line") or parsed_input.get("end_index")
            elif isinstance(input, dict):
                filename = input.get("filename")
                # å…¼å®¹ start_line/end_line å’Œ start_index/end_index
                start_line = input.get("start_line") or input.get("start_index")
                end_line = input.get("end_line") or input.get("end_index")
                
        except Exception as e:
            log.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if not filename or not start_line or not end_line:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œæ–‡ä»¶åã€èµ·å§‹è¡Œå·å’Œç»“æŸè¡Œå·ä¸èƒ½ä¸ºç©ºï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if start_line > end_line:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å¤§äºç»“æŸè¡Œå·ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if start_line < 1:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if end_line < 1:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œç»“æŸè¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

        try:
            file_path = os.path.join(self.upload_path, filename)
            if not os.path.exists(file_path):
                return f"{filename}æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ ¸å®æ–‡ä»¶åç§°æ˜¯å¦æ­£ç¡®ã€‚"
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    return f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ï¼Œç¼–ç ä¸æ”¯æŒ"
            
            lines = content.split('\n')
            if end_line - start_line > 50:
                end_line = start_line + 50
            selected_lines = lines[start_line-1:end_line]
            return json.dumps({"content": selected_lines}, ensure_ascii=False, indent=2)
        except Exception as e:
            log.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}"

    def _read_file_content_tool(self):
        """è¯»å–æ–‡ä»¶å†…å®¹å·¥å…·"""
        @tool
        def read_file_content_tool_func(input: Dict[str, Any] | str) -> str:
            """è¯»å–æŒ‡å®šæ–‡ä»¶çš„è¡Œå·èŒƒå›´å†…çš„å†…å®¹"""
            return self.read_file_by_lines(input)
        return read_file_content_tool_func
    

    def _add_fragment_meta_tool(self):
        """æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®å·¥å…·"""
        @tool
        def add_fragment_meta_tool_func(input: List[AddFragmentMetaToolInput] | AddFragmentMetaToolInput | str) -> str:
            """æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®"""
            try:
                fragment_meta_list: List[DocumentFragmentMeta] = []
                # è§£æè¾“å…¥å‚æ•°
                if isinstance(input, str):
                    try:
                        parsed_input = json.loads(input.strip())
                    except json.JSONDecodeError:
                        # ä¿®å¤JSONæ ¼å¼é—®é¢˜
                        fixed_input = self._fix_json_format(input)
                        parsed_input = json.loads(fixed_input)
                    
                    # å¤„ç†è§£æåçš„æ•°æ®
                    if "fragments" in parsed_input and isinstance(parsed_input["fragments"], list):
                        for item in parsed_input["fragments"]:
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=item.get("filename"), 
                                start_line=item.get("start_line"), 
                                end_line=item.get("end_line")
                            ))
                    else:
                        fragment_meta_list.append(DocumentFragmentMeta(
                            filename=parsed_input.get("filename"), 
                            start_line=parsed_input.get("start_line"), 
                            end_line=parsed_input.get("end_line")
                        ))
                        
                elif isinstance(input, AddFragmentMetaToolInput):
                    fragment_meta_list.append(DocumentFragmentMeta(
                        filename=input.filename, 
                        start_line=input.start_line, 
                        end_line=input.end_line
                    ))
                elif isinstance(input, list):
                    for item in input:
                        if isinstance(item, AddFragmentMetaToolInput):
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=item.filename, 
                                start_line=item.start_line, 
                                end_line=item.end_line
                            ))
                        elif isinstance(item, dict):
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=item.get("filename"), 
                                start_line=item.get("start_line"), 
                                end_line=item.get("end_line")
                            ))
                
                # éªŒè¯å‚æ•°
                for fragment in fragment_meta_list:
                    if not fragment.filename or not fragment.start_line or not fragment.end_line:
                        return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œæ–‡ä»¶åã€èµ·å§‹è¡Œå·å’Œç»“æŸè¡Œå·ä¸èƒ½ä¸ºç©ºï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
                    if fragment.start_line > fragment.end_line:
                        return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å¤§äºç»“æŸè¡Œå·ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
                    if fragment.start_line < 1 or fragment.end_line < 1:
                        return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

                # ä¿å­˜ç‰‡æ®µ
                self.fragments_meta.extend(fragment_meta_list)
                log.info(f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®æˆåŠŸ: {len(fragment_meta_list)} ä¸ªç‰‡æ®µ")
                return f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®æˆåŠŸï¼Œå…±ä¿å­˜ {len(fragment_meta_list)} ä¸ªç‰‡æ®µ"
                
            except Exception as e:
                log.error(f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®å¤±è´¥: {e}")
                return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        return add_fragment_meta_tool_func

    def clear_fragments_meta(self):
        """æ¸…ç©ºæ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨"""
        self.fragments_meta = []

    def get_fragments_meta(self) -> List[DocumentFragmentMeta]:
        """è·å–æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨"""
        return self.fragments_meta

    def update_fragments(self):
        """æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨"""
        self.fragments = [] # æ¸…ç©ºæ—§çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        try:
            for fragment_meta in self.fragments_meta:
                content = self.read_file_by_lines({
                    "filename": fragment_meta.filename,
                    "start_line": fragment_meta.start_line,
                    "end_line": fragment_meta.end_line
                })
                if content:
                    self.fragments.append(DocumentFragment(
                        filename=fragment_meta.filename,
                        content=content,
                        start_line=fragment_meta.start_line,
                        end_line=fragment_meta.end_line
                    ))
        except Exception as e:
            log.error(f"æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨å¤±è´¥: {e}")
            return f"æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨å¤±è´¥: {e}"

    def get_fragments(self) -> List[DocumentFragment]:
        """è·å–æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨"""
        return self.fragments

class AgentLoggingCallback(BaseCallbackHandler):
    """Agentæ‰§è¡Œè¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—è®°å½•å›è°ƒ"""
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•LLMå¼€å§‹å¤„ç†"""
        log.info(f"ğŸ§  LLM Start: å¤„ç† {len(prompts)} ä¸ªæç¤º")
        for i, prompt in enumerate(prompts):
            log.info(f"  æç¤º {i+1}: {prompt[:200]}...")
    
    def on_llm_end(self, response, **kwargs):
        """è®°å½•LLMå¤„ç†å®Œæˆ"""
        log.info(f"ğŸ§  LLM End: {response.generations[0][0].text[:200]}...")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        """è®°å½•Chainå¼€å§‹"""
        if serialized:
            log.info(f"ğŸ”— Chain Start: {serialized.get('name', 'Unknown')}")
        else:
            log.info(f"ğŸ”— Chain Start: Unknown")
        log.info(f"  è¾“å…¥: {inputs}")
    
    def on_chain_end(self, outputs, **kwargs):
        """è®°å½•Chainç»“æŸ"""
        log.info(f"ğŸ”— Chain End: {outputs}")
    
    def on_chain_error(self, error, **kwargs):
        """è®°å½•Chainé”™è¯¯"""
        log.error(f"âŒ Chain Error: {error}")
    
    def on_agent_action(self, action, **kwargs):
        """è®°å½•Agentæ‰§è¡Œçš„åŠ¨ä½œ"""
        log.info(f"ğŸ¤– Agent Action: {action.tool}")
        log.info(f"  å·¥å…·è¾“å…¥: {action.tool_input}")
        log.info(f"  æ—¥å¿—: {action.log}")
    
    def on_agent_finish(self, finish, **kwargs):
        """è®°å½•Agentå®Œæˆæ‰§è¡Œ"""
        log.info(f"âœ… Agent Finish: {finish.return_values}")
        log.info(f"  æ—¥å¿—: {finish.log}")
    
    def on_tool_start(self, serialized, input_str, **kwargs):
        """è®°å½•å·¥å…·å¼€å§‹æ‰§è¡Œ"""
        tool_name = serialized.get('name', 'Unknown')
        log.info(f"ğŸ”§ Tool Start: {tool_name}")
        log.info(f"  å·¥å…·è¾“å…¥: {input_str}")
        log.info(f"  å·¥å…·ç±»å‹: {type(serialized)}")
        log.info(f"  åºåˆ—åŒ–æ•°æ®: {serialized}")
    
    def on_tool_end(self, output, **kwargs):
        """è®°å½•å·¥å…·æ‰§è¡Œå®Œæˆ"""
        log.info(f"âœ… Tool End: {output}")
    
    def on_tool_error(self, error, **kwargs):
        """è®°å½•å·¥å…·æ‰§è¡Œé”™è¯¯"""
        log.error(f"âŒ Tool Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_chain_error(self, error, **kwargs):
        """è®°å½•Chainé”™è¯¯"""
        log.error(f"âŒ Chain Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_text(self, text, **kwargs):
        """è®°å½•æ–‡æœ¬è¾“å‡º"""
        log.info(f"ğŸ“ Text: {text}")
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•LLMå¼€å§‹å¤„ç†"""
        log.info(f"ğŸ§  LLM Start: å¤„ç† {len(prompts)} ä¸ªæç¤º")
        for i, prompt in enumerate(prompts):
            log.info(f"  æç¤º {i+1}: {prompt[:200]}...")
    
    def on_llm_end(self, response, **kwargs):
        """è®°å½•LLMå¤„ç†å®Œæˆ"""
        if response.generations and response.generations[0]:
            log.info(f"ğŸ§  LLM End: {response.generations[0][0].text[:200]}...")
        else:
            log.info(f"ğŸ§  LLM End: æ— å“åº”å†…å®¹")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")


# æµ‹è¯•å‘½ä»¤ï¼Œæ ¹ç›®å½•è·¯å¾„è¿è¡Œï¼šuv run python -m src.knowledge_qa.llms.reader_llm
if __name__ == "__main__":
    reader_llm = ReaderLLM()
    # result = reader_llm.generate("éŸ©ç«‹æ˜¯å¦‚ä½•è¿›å…¥ä¸ƒç„é—¨çš„ï¼Ÿè®°åå¼Ÿå­åˆæ¬¡è€ƒéªŒåŒ…å«å“ªäº›å…³é”®è·¯æ®µä¸ç¯èŠ‚ï¼Ÿ")
    result = reader_llm.generate("å½“ä¸€ä¸ªç›®æ ‡å¤„äº Grappledï¼ˆè¢«æ“’æŠ±/ç¼ ä½ï¼‰çŠ¶æ€æ—¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿåˆ—å‡ºè¯¥çŠ¶æ€å¯¹ç›®æ ‡çš„å…·ä½“æœºæ¢°æ•ˆæœã€‚")
    print(result)
    print("=" * 50)
    print("æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨:")
    reader_llm.update_fragments()
    print(reader_llm.get_fragments())