"""é˜…è¯»æœ¬åœ°æ–‡ä»¶èµ„æ–™çš„å·¥å…·æ¨¡å‹"""

import os 
import json
import re
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain import hub
from langchain_core.tools import tool, Tool
from langchain_core.prompts import PromptTemplate
from langchain_core.callbacks import BaseCallbackHandler
from pydantic import BaseModel, Field
from ..config import settings
from ..log_manager import log


class DocumentFragmentMeta(BaseModel):
    """æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®ï¼ˆä¸å«å†…å®¹ï¼ŒèŠ‚çœtokenï¼‰"""
    filename: str
    start_line: int
    end_line: int

class DocumentFragment(BaseModel):
    """æ–‡æ¡£ç‰‡æ®µ"""
    filename: str
    content: str
    start_line: int
    end_line: int

class SearchKeywordToolInput(BaseModel):
    keyword: str = Field(..., description="å…³é”®è¯")
    filename: str = Field(..., description="æ–‡ä»¶å")
    limit: int = Field(default=300, description="æœ€å¤§è¿”å›ç»“æœæ•°")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"keyword": "å…³é”®è¯", "filename": "æ–‡ä»¶å", "limit": 300}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "keyword": "å…³é”®è¯ (å¿…å¡«)",
            "filename": "æ–‡ä»¶å (å¿…å¡«)", 
            "limit": "æœ€å¤§è¿”å›ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤300)"
        }

class ReadFileContentToolInput(BaseModel):
    filename: str = Field(..., description="æ–‡ä»¶å")
    start_index: int = Field(..., description="èµ·å§‹è¡Œå·")
    end_index: int = Field(..., description="ç»“æŸè¡Œå·")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"filename": "æ–‡ä»¶å", "start_index": 1, "end_index": 10}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "filename": "æ–‡ä»¶å (å¿…å¡«)",
            "start_index": "èµ·å§‹è¡Œå· (å¿…å¡«)",
            "end_index": "ç»“æŸè¡Œå· (å¿…å¡«)"
        }

class AddFragmentMetaToolInput(BaseModel):
    filename: str = Field(..., description="æ–‡ä»¶å")
    start_line: int = Field(..., description="èµ·å§‹è¡Œå·")
    end_line: int = Field(..., description="ç»“æŸè¡Œå·")
    
    @classmethod
    def get_example_format(cls) -> str:
        """è·å–å‚æ•°æ ¼å¼ç¤ºä¾‹"""
        return '{"filename": "æ–‡ä»¶å", "start_line": 1, "end_line": 10}'
    
    @classmethod
    def get_schema_dict(cls) -> dict:
        """è·å–å‚æ•°æ ¼å¼çš„å­—å…¸è¡¨ç¤º"""
        return {
            "filename": "æ–‡ä»¶å (å¿…å¡«)",
            "start_line": "èµ·å§‹è¡Œå· (å¿…å¡«)",
            "end_line": "ç»“æŸè¡Œå· (å¿…å¡«)"
        }

class ReaderLLM:
    """é˜…è¯»æœ¬åœ°æ–‡ä»¶èµ„æ–™çš„å·¥å…·æ¨¡å‹"""

    def __init__(self):
        self.upload_path = settings.upload_temp_path
        self.tools = self._create_all_tools()
        self.llm = ChatOpenAI(
            openai_api_key=settings.siliconcloud_api_key,
            openai_api_base=settings.siliconcloud_api_base,
            model=settings.llm_model,
            temperature=settings.llm_temperature
        )

        # æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨
        self.fragments_meta = []
        # å®Œæ•´æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ˆç¼–ç¨‹è·å–ï¼Œä¿è¯ç²¾å‡†åŒ¹é…ï¼‰
        self.fragments = []
        
        # åˆ›å»ºæ—¥å¿—è®°å½•å›è°ƒï¼ˆè°ƒè¯•ï¼‰
        self.agent_callback = AgentLoggingCallback()
        
        for tool in self.tools:
            log.info(f"  - {tool.name}: {tool.description}")

        self.agent = create_react_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self._get_prompt_template()
        )
        
        # åˆ›å»ºAgentExecutoræ¥æ‰§è¡Œagent
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
            callbacks=[self.agent_callback],
            max_iterations=10,
            early_stopping_method="generate"
        )
        log.info("Agent å’Œ AgentExecutor åˆ›å»ºå®Œæˆ")
    
    def _get_prompt_template(self) -> PromptTemplate:
        """åˆ›å»ºReAct Agentçš„promptæ¨¡æ¿ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼‰,å®˜æ–¹å»ºè®®react agent çš„prompt ç”¨è‹±æ–‡"""
        return PromptTemplate.from_template("""
Answer the following questions as best you can. You have access to the following tools:

{tools}

**Tool Usage Instructions:**
- list_files_tool_func(): List all available files, no parameters needed
- search_keyword_tool_func(keyword, filename, limit=300): Search for keywords in files and return relevant lines with context
  Purpose: Find specific content related to keywords, returns line numbers and surrounding context
  Parameters: keyword (search term), filename (file name), limit (max results, default 300)
  Use this when: You need to find specific information or content within a file
- read_file_content_tool_func(filename, start_index, end_index): Read file content by line range
  Purpose: Get detailed content from specific line ranges, useful after finding relevant lines with search
  Parameters: filename (file name), start_index (start line number), end_index (end line number)
  Use this when: You need to read more detailed content after finding relevant lines with search_keyword_tool_func
- add_fragment_meta_tool_func(fragments): Add document fragment metadata to the system
  Purpose: Store relevant document fragments for future reference and context building
  Parameters: fragments (array of fragment objects with filename, start_line, end_line)
  Use this when: You find relevant content that should be saved for the user's question
  Note: You can call this multiple times and can input single or multiple fragments as an array
  **CRITICAL**: This is your PRIMARY GOAL - save ALL relevant fragments you discover
  Format examples:
    Single fragment: {{"filename": "file.txt", "start_line": 10, "end_line": 20}}
    Multiple fragments: {{"fragments": [{{"filename": "file.txt", "start_line": 10, "end_line": 20}}, {{"filename": "file.txt", "start_line": 30, "end_line": 40}}]}}

**Workflow Guidelines:**
1. First, use list_files_tool_func() to see available files
2. Then use search_keyword_tool_func() to find relevant content
3. If you need more context, use read_file_content_tool_func() to read specific line ranges
4. **CRITICAL**: Use add_fragment_meta_tool_func() to save ALL relevant fragments you find (this is your PRIMARY GOAL)
5. Always provide a complete answer based on the information you gather

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action (use the correct parameter names as shown above)
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

**Important Notes:**
- **YOUR MAIN OBJECTIVE**: Find and save ALL relevant document fragments using add_fragment_meta_tool_func
- After using search_keyword_tool_func, if you find relevant content, you can use read_file_content_tool_func to get more context
- **MANDATORY**: Use add_fragment_meta_tool_func to save important fragments you discover (supports both single and multiple fragments)
- You can call add_fragment_meta_tool_func multiple times throughout your search process
- **PRIORITY**: Saving relevant fragments is more important than just answering the question
- Always follow the exact format: Thought -> Action -> Action Input -> Observation
- If you have enough information to answer the question, proceed to Final Answer
- Never skip the Action line - always include it when you want to use a tool

Begin!

Question: {input}
Thought:{agent_scratchpad}
        """)
    
    def generate(self, query: str) -> str:
        """æ ¹æ®é—®é¢˜ç”Ÿæˆå›ç­”"""
        log.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {query}")
        try:
            result = self.agent_executor.invoke({"input": query})
            log.info("æŸ¥è¯¢å¤„ç†å®Œæˆ")
            return result
        except Exception as e:
            log.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            log.error(f"é”™è¯¯ç±»å‹: {type(e)}")
            log.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            log.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise

    def _create_all_tools(self) -> List[Tool]:
        """åˆ›å»ºæ‰€æœ‰å·¥å…·"""
        tools = [
            self._list_files_tool(),
            self._search_keyword_tool(),
            self._read_file_content_tool(),
            self._add_fragment_meta_tool()
        ]
        log.info(f"åˆ›å»ºäº† {len(tools)} ä¸ªå·¥å…·")
        for i, tool in enumerate(tools):
            log.info(f"  å·¥å…· {i+1}: {tool.name}")
        return tools

    def _list_files_tool(self):
        """åˆ—å‡ºæ–‡ä»¶å·¥å…·"""
        @tool
        def list_files_tool_func() -> str:
            """åˆ—å‡ºä¸Šä¼ æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
            try:
                log.info(f"æ­£åœ¨æ£€æŸ¥ç›®å½•: {self.upload_path}")
                if not os.path.exists(self.upload_path):
                    log.warning(f"ç›®å½•ä¸å­˜åœ¨: {self.upload_path}")
                    return f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.upload_path}"

                files = []
                for filename in os.listdir(self.upload_path):
                    file_path = os.path.join(self.upload_path, filename)
                    if os.path.isfile(file_path):
                        files.append(filename)
                
                result = json.dumps({"files": files}, ensure_ascii=False, indent=2)
                log.info(f"æ‰¾åˆ°æ–‡ä»¶: {result}")
                return result
            except Exception as e:
                log.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
                return f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}"
        return list_files_tool_func

    def _search_keyword_tool(self):
        """æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹å·¥å…·"""
        @tool
        def search_keyword_tool_func(input: SearchKeywordToolInput | str) -> str:
            """åœ¨æŒ‡å®šæ–‡ä»¶ä¸­æœç´¢å…³é”®è¯ç›¸å…³å†…å®¹"""
            # å‚æ•°è§£æ
            try:
                if isinstance(input, str):
                    input = json.loads(input)
                    keyword = input.get("keyword")
                    filename = input.get("filename")
                    limit = input.get("limit")
                else:
                    keyword = input.keyword
                    filename = input.filename
                    limit = input.limit
            except Exception as e:
                log.error(f"æœç´¢å…³é”®è¯å¤±è´¥: {e}")
                return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {SearchKeywordToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

            try:
                # keyword åˆ‡åˆ†, è¿™é‡Œç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ‡åˆ†ï¼Œç©ºæ ¼ï¼Œé€—å·å¸¸è§çš„åˆ†éš”ç¬¦
                keywords = re.split(r'[,\s]+', keyword)

                file_path = os.path.join(self.upload_path, filename)
                if not os.path.exists(file_path):
                    return f"{filename}æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ ¸å®æ–‡ä»¶åç§°æ˜¯å¦æ­£ç¡®ã€‚"
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                        break
                    except UnicodeDecodeError:
                        return f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ï¼Œç¼–ç ä¸æ”¯æŒ"

                lines = content.split('\n')
                relevant_lines = []
                for i, line in enumerate(lines, 1):
                    if any(keyword.lower() in line.lower() for keyword in keywords):
                        relevant_lines.append({
                            "line_number": i,
                            "content": line.strip()
                        })
                if limit is None:
                    limit = 300  # é»˜è®¤å€¼
                if len(relevant_lines) > limit:
                    relevant_lines = self._smart_sample_lines(relevant_lines, limit)
                result = {
                    "filename": filename,
                    "total_matches": len(relevant_lines),
                    "relevant_lines": relevant_lines
                }
                return json.dumps(result, ensure_ascii=False, indent=2)
                log.info(f"æœç´¢å…³é”®è¯ {keyword} åœ¨æ–‡ä»¶ {filename} ä¸­æ‰¾åˆ° {len(relevant_lines)} è¡Œç›¸å…³å†…å®¹ï¼Œè¿”å›å‰ {len(relevant_lines)} è¡Œ")
            except Exception as e:
                log.error(f"æœç´¢å…³é”®è¯å¤±è´¥: {e}")
                return f"æœç´¢å…³é”®è¯å¤±è´¥: {e}"
        return search_keyword_tool_func

    def _smart_sample_lines(self, relevant_lines: List[Dict], limit: int) -> List[Dict]:
        """æ™ºèƒ½é‡‡æ ·ç›¸å…³è¡Œï¼Œç¡®ä¿è¦†ç›–å…¨æ–‡çš„ä¸åŒéƒ¨åˆ†"""
        if limit is None:
            limit = 300  # é»˜è®¤å€¼
        if len(relevant_lines) <= limit:
            return relevant_lines
        
        # æŒ‰è¡Œå·æ’åº
        sorted_lines = sorted(relevant_lines, key=lambda x: x['line_number'])
        total_lines = len(sorted_lines)
        
        if total_lines <= limit:
            return sorted_lines
        
        # è®¡ç®—é‡‡æ ·é—´éš”ï¼Œç¡®ä¿è¦†ç›–å…¨æ–‡
        step = total_lines / limit
        
        sampled_lines = []
        for i in range(limit):
            # è®¡ç®—é‡‡æ ·ä½ç½®ï¼Œç¡®ä¿å‡åŒ€åˆ†å¸ƒ
            index = int(i * step)
            if index < total_lines:
                sampled_lines.append(sorted_lines[index])
        
        # ç¡®ä¿åŒ…å«å¼€å¤´å’Œç»“å°¾çš„é‡è¦å†…å®¹
        if sorted_lines[0] not in sampled_lines:
            sampled_lines[0] = sorted_lines[0]
        if sorted_lines[-1] not in sampled_lines:
            sampled_lines[-1] = sorted_lines[-1]
        
        # æŒ‰è¡Œå·é‡æ–°æ’åº
        return sorted(sampled_lines, key=lambda x: x['line_number'])

    def read_file_by_lines(self, input: Dict[str, Any] | str) -> str:
        """è¯»å–æŒ‡å®šæ–‡ä»¶çš„è¡Œå·èŒƒå›´å†…çš„å†…å®¹ï¼Œè¿”å›å­—ç¬¦ä¸²"""
        # å‚æ•°è§£æ
        try:
            if isinstance(input, str):
                # æ¸…ç†è¾“å…¥å­—ç¬¦ä¸²ï¼Œç§»é™¤å¯èƒ½çš„ä¸å¯è§å­—ç¬¦
                cleaned_input = input.strip()
                input = json.loads(cleaned_input)
                filename = input.get("filename")
                # å…¼å®¹ start_line/end_line å’Œ start_index/end_index
                start_index = input.get("start_index") or input.get("start_line")
                end_index = input.get("end_index") or input.get("end_line")
            elif isinstance(input, dict):
                filename = input.get("filename")
                # å…¼å®¹ start_line/end_line å’Œ start_index/end_index
                start_index = input.get("start_index") or input.get("start_line")
                end_index = input.get("end_index") or input.get("end_line")
            else:
                filename = input.filename
                # å…¼å®¹ start_line/end_line å’Œ start_index/end_index
                start_index = getattr(input, 'start_index', None) or getattr(input, 'start_line', None)
                end_index = getattr(input, 'end_index', None) or getattr(input, 'end_line', None)
        except Exception as e:
            log.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            log.error(f"è¾“å…¥å‚æ•°: {repr(input)}")
            log.error(f"è¾“å…¥ç±»å‹: {type(input)}")
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if not filename or not start_index or not end_index:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œæ–‡ä»¶åã€èµ·å§‹è¡Œå·å’Œç»“æŸè¡Œå·ä¸èƒ½ä¸ºç©ºï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if start_index > end_index:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å¤§äºç»“æŸè¡Œå·ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if start_index < 1:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
        if end_index < 1:
            return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œç»“æŸè¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {ReadFileContentToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

        try:
            file_path = os.path.join(self.upload_path, filename)
            if not os.path.exists(file_path):
                return f"{filename}æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ ¸å®æ–‡ä»¶åç§°æ˜¯å¦æ­£ç¡®ã€‚"
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    return f"æ— æ³•è¯»å–æ–‡ä»¶ {filename}ï¼Œç¼–ç ä¸æ”¯æŒ"
            lines = content.split('\n')
            return json.dumps({"content": lines[start_index:end_index]}, ensure_ascii=False, indent=2)
            log.info(f"è¯»å–æ–‡ä»¶ {filename} å†…å®¹ï¼Œè¿”å›ç¬¬ {start_index} è¡Œåˆ°ç¬¬ {end_index} è¡Œ")
        except Exception as e:
            log.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}"

    def _read_file_content_tool(self):
        """è¯»å–æ–‡ä»¶å†…å®¹å·¥å…·"""
        @tool
        def read_file_content_tool_func(input: Dict[str, Any] | str) -> str:
            """è¯»å–æŒ‡å®šæ–‡ä»¶çš„è¡Œå·èŒƒå›´å†…çš„å†…å®¹"""
            return self.read_file_by_lines(input)
        return read_file_content_tool_func
    

    def _add_fragment_meta_tool(self):
        """æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®å·¥å…·"""
        @tool
        def add_fragment_meta_tool_func(input: List[AddFragmentMetaToolInput] | AddFragmentMetaToolInput | str) -> str:
            """æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®"""
            fragment_meta_list: List[DocumentFragmentMeta] = []
            
            # å‚æ•°è§£æ
            try:
                if isinstance(input, str):
                    # è§£æJSONå­—ç¬¦ä¸²
                    parsed_input = json.loads(input)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯fragmentsæ•°ç»„æ ¼å¼
                    if "fragments" in parsed_input and isinstance(parsed_input["fragments"], list):
                        # å¤„ç†fragmentsæ•°ç»„æ ¼å¼
                        for item in parsed_input["fragments"]:
                            filename = item.get("filename")
                            start_line = item.get("start_line")
                            end_line = item.get("end_line")
                            if filename and start_line and end_line:
                                fragment_meta_list.append(DocumentFragmentMeta(
                                    filename=filename, 
                                    start_line=start_line, 
                                    end_line=end_line
                                ))
                    else:
                        # å¤„ç†å•ä¸ªç‰‡æ®µæ ¼å¼
                        filename = parsed_input.get("filename")
                        start_line = parsed_input.get("start_line")
                        end_line = parsed_input.get("end_line")
                        if filename and start_line and end_line:
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=filename, 
                                start_line=start_line, 
                                end_line=end_line
                            ))
                elif isinstance(input, AddFragmentMetaToolInput):
                    # å¤„ç†Pydanticæ¨¡å‹
                    fragment_meta_list.append(DocumentFragmentMeta(
                        filename=input.filename, 
                        start_line=input.start_line, 
                        end_line=input.end_line
                    ))
                elif isinstance(input, list):
                    # å¤„ç†åˆ—è¡¨æ ¼å¼
                    for item in input:
                        if isinstance(item, AddFragmentMetaToolInput):
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=item.filename, 
                                start_line=item.start_line, 
                                end_line=item.end_line
                            ))
                        elif isinstance(item, dict):
                            fragment_meta_list.append(DocumentFragmentMeta(
                                filename=item.get("filename"), 
                                start_line=item.get("start_line"), 
                                end_line=item.get("end_line")
                            ))
            except Exception as e:
                log.error(f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®å¤±è´¥: {e}")
                log.error(f"è¾“å…¥å‚æ•°: {input}")
                log.error(f"è¾“å…¥ç±»å‹: {type(input)}")
                return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
            
            # éªŒè¯å‚æ•°
            for fragment in fragment_meta_list:
                if not fragment.filename or not fragment.start_line or not fragment.end_line:
                    return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œæ–‡ä»¶åã€èµ·å§‹è¡Œå·å’Œç»“æŸè¡Œå·ä¸èƒ½ä¸ºç©ºï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
                if fragment.start_line > fragment.end_line:
                    return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å¤§äºç»“æŸè¡Œå·ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
                if fragment.start_line < 1:
                    return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œèµ·å§‹è¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"
                if fragment.end_line < 1:
                    return f"è¾“å…¥å‚æ•°æœ‰è¯¯ï¼Œç»“æŸè¡Œå·ä¸èƒ½å°äº1ï¼Œè¯·å‚è€ƒæ ¼å¼: {AddFragmentMetaToolInput.get_example_format()}ï¼Œé‡æ–°æ£€æŸ¥åé‡è¯•ã€‚"

            self.fragments_meta.extend(fragment_meta_list)
            log.info(f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®æˆåŠŸ: {len(fragment_meta_list)} ä¸ªç‰‡æ®µ")
            return f"æ·»åŠ æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®æˆåŠŸï¼Œå…±ä¿å­˜ {len(fragment_meta_list)} ä¸ªç‰‡æ®µ"
        return add_fragment_meta_tool_func

    def clear_fragments_meta(self):
        """æ¸…ç©ºæ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨"""
        self.fragments_meta = []

    def get_fragments_meta(self) -> List[DocumentFragmentMeta]:
        """è·å–æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨"""
        return self.fragments_meta

    def update_fragments(self):
        """æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨"""
        self.fragments = [] # æ¸…ç©ºæ—§çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        try:
            for fragment_meta in self.fragments_meta:
                content = self.read_file_by_lines({
                    "filename": fragment_meta.filename,
                    "start_index": fragment_meta.start_line,
                    "end_index": fragment_meta.end_line
                })
                if content:
                    self.fragments.append(DocumentFragment(
                        filename=fragment_meta.filename,
                        content=content,
                        start_line=fragment_meta.start_line,
                        end_line=fragment_meta.end_line
                    ))
        except Exception as e:
            log.error(f"æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨å¤±è´¥: {e}")
            return f"æ›´æ–°æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨å¤±è´¥: {e}"

    def get_fragments(self) -> List[DocumentFragment]:
        """è·å–æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨"""
        return self.fragments

class AgentLoggingCallback(BaseCallbackHandler):
    """Agentæ‰§è¡Œè¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—è®°å½•å›è°ƒ"""
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•LLMå¼€å§‹å¤„ç†"""
        log.info(f"ğŸ§  LLM Start: å¤„ç† {len(prompts)} ä¸ªæç¤º")
        for i, prompt in enumerate(prompts):
            log.info(f"  æç¤º {i+1}: {prompt[:200]}...")
    
    def on_llm_end(self, response, **kwargs):
        """è®°å½•LLMå¤„ç†å®Œæˆ"""
        log.info(f"ğŸ§  LLM End: {response.generations[0][0].text[:200]}...")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        """è®°å½•Chainå¼€å§‹"""
        if serialized:
            log.info(f"ğŸ”— Chain Start: {serialized.get('name', 'Unknown')}")
        else:
            log.info(f"ğŸ”— Chain Start: Unknown")
        log.info(f"  è¾“å…¥: {inputs}")
    
    def on_chain_end(self, outputs, **kwargs):
        """è®°å½•Chainç»“æŸ"""
        log.info(f"ğŸ”— Chain End: {outputs}")
    
    def on_chain_error(self, error, **kwargs):
        """è®°å½•Chainé”™è¯¯"""
        log.error(f"âŒ Chain Error: {error}")
    
    def on_agent_action(self, action, **kwargs):
        """è®°å½•Agentæ‰§è¡Œçš„åŠ¨ä½œ"""
        log.info(f"ğŸ¤– Agent Action: {action.tool}")
        log.info(f"  å·¥å…·è¾“å…¥: {action.tool_input}")
        log.info(f"  æ—¥å¿—: {action.log}")
    
    def on_agent_finish(self, finish, **kwargs):
        """è®°å½•Agentå®Œæˆæ‰§è¡Œ"""
        log.info(f"âœ… Agent Finish: {finish.return_values}")
        log.info(f"  æ—¥å¿—: {finish.log}")
    
    def on_tool_start(self, serialized, input_str, **kwargs):
        """è®°å½•å·¥å…·å¼€å§‹æ‰§è¡Œ"""
        tool_name = serialized.get('name', 'Unknown')
        log.info(f"ğŸ”§ Tool Start: {tool_name}")
        log.info(f"  å·¥å…·è¾“å…¥: {input_str}")
        log.info(f"  å·¥å…·ç±»å‹: {type(serialized)}")
        log.info(f"  åºåˆ—åŒ–æ•°æ®: {serialized}")
    
    def on_tool_end(self, output, **kwargs):
        """è®°å½•å·¥å…·æ‰§è¡Œå®Œæˆ"""
        log.info(f"âœ… Tool End: {output}")
    
    def on_tool_error(self, error, **kwargs):
        """è®°å½•å·¥å…·æ‰§è¡Œé”™è¯¯"""
        log.error(f"âŒ Tool Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_chain_error(self, error, **kwargs):
        """è®°å½•Chainé”™è¯¯"""
        log.error(f"âŒ Chain Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")
    
    def on_text(self, text, **kwargs):
        """è®°å½•æ–‡æœ¬è¾“å‡º"""
        log.info(f"ğŸ“ Text: {text}")
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """è®°å½•LLMå¼€å§‹å¤„ç†"""
        log.info(f"ğŸ§  LLM Start: å¤„ç† {len(prompts)} ä¸ªæç¤º")
        for i, prompt in enumerate(prompts):
            log.info(f"  æç¤º {i+1}: {prompt[:200]}...")
    
    def on_llm_end(self, response, **kwargs):
        """è®°å½•LLMå¤„ç†å®Œæˆ"""
        if response.generations and response.generations[0]:
            log.info(f"ğŸ§  LLM End: {response.generations[0][0].text[:200]}...")
        else:
            log.info(f"ğŸ§  LLM End: æ— å“åº”å†…å®¹")
    
    def on_llm_error(self, error, **kwargs):
        """è®°å½•LLMé”™è¯¯"""
        log.error(f"âŒ LLM Error: {error}")
        log.error(f"  é”™è¯¯ç±»å‹: {type(error)}")
        log.error(f"  é”™è¯¯è¯¦æƒ…: {str(error)}")


# æµ‹è¯•å‘½ä»¤ï¼Œæ ¹ç›®å½•è·¯å¾„è¿è¡Œï¼šuv run python -m src.knowledge_qa.llms.reader_llm
if __name__ == "__main__":
    reader_llm = ReaderLLM()
    # result = reader_llm.generate("éŸ©ç«‹æ˜¯å¦‚ä½•è¿›å…¥ä¸ƒç„é—¨çš„ï¼Ÿè®°åå¼Ÿå­åˆæ¬¡è€ƒéªŒåŒ…å«å“ªäº›å…³é”®è·¯æ®µä¸ç¯èŠ‚ï¼Ÿ")
    result = reader_llm.generate("å½“ä¸€ä¸ªç›®æ ‡å¤„äº Grappledï¼ˆè¢«æ“’æŠ±/ç¼ ä½ï¼‰çŠ¶æ€æ—¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿåˆ—å‡ºè¯¥çŠ¶æ€å¯¹ç›®æ ‡çš„å…·ä½“æœºæ¢°æ•ˆæœã€‚")
    print(result)
    print("=" * 50)
    print("æ–‡æ¡£ç‰‡æ®µå…ƒæ•°æ®åˆ—è¡¨:")
    print(reader_llm.fragments_meta)