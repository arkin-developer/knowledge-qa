"""å‘é‡å­˜å‚¨ç®¡ç†å™¨ - FAISSå‘é‡æ•°æ®åº“çš„å®šåˆ¶åŒ–é…ç½®"""

from typing import Optional, List, Dict, Any
from pathlib import Path
import shutil
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langsmith import traceable

from .config import settings
from .log_manager import log


class VectorStore:
    """FAISSå‘é‡å­˜å‚¨ç®¡ç†å™¨ - æ”¯æŒå®šåˆ¶åŒ–é…ç½®"""
    
    def __init__(
        self,
        embeddings: Optional[OpenAIEmbeddings] = None,
        vector_store_path: Optional[str] = None,
        index_name: str = "index",
        allow_dangerous_deserialization: bool = True
    ):
        """åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            embeddings: åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            vector_store_path: å‘é‡å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            index_name: ç´¢å¼•åç§°
            allow_dangerous_deserialization: æ˜¯å¦å…è®¸å±é™©çš„ååºåˆ—åŒ–
        """
        self.index_name = index_name
        self.allow_dangerous_deserialization = allow_dangerous_deserialization
        
        # è®¾ç½®è·¯å¾„
        self.vector_store_path = vector_store_path or settings.vector_store_path
        self.persist_dir = Path(self.vector_store_path)
        
        # è®¾ç½®åµŒå…¥æ¨¡å‹
        if embeddings is None:
            self.embeddings = OpenAIEmbeddings(
                openai_api_key=settings.siliconcloud_api_key,
                openai_api_base=settings.siliconcloud_api_base,
                model=settings.embedding_model
            )
        else:
            self.embeddings = embeddings
            
        # å‘é‡å­˜å‚¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._vector_store: Optional[FAISS] = None
        self._load_existing_vector_store()
    
    @property
    def vector_store(self) -> Optional[FAISS]:
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        return self._vector_store
    
    def _load_existing_vector_store(self) -> None:
        """åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨"""
        try:
            index_path = self.persist_dir / f"{self.index_name}.faiss"
            if index_path.exists():
                self._vector_store = FAISS.load_local(
                    str(self.persist_dir),
                    self.embeddings,
                    allow_dangerous_deserialization=self.allow_dangerous_deserialization,
                    index_name=self.index_name
                )
                log.info(f"âœ… å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ: {self.persist_dir}")
            else:
                log.info(f"â„¹ï¸ æœªæ‰¾åˆ°ç°æœ‰å‘é‡å­˜å‚¨: {index_path}")
        except Exception as e:
            log.warning(f"âš ï¸ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
    
    @traceable(name="create_vector_store")
    def create_vector_store(self, documents: List[Document]) -> FAISS:
        """åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨"""
        if not documents:
            raise ValueError("æ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        log.info(f"ğŸš€ åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {len(documents)}")
        self._vector_store = FAISS.from_documents(documents, self.embeddings)
        log.info("âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ")
        return self._vector_store
    
    @traceable(name="add_documents")
    def add_documents(
        self, 
        documents: List[Document], 
        batch_size: int = 10,
        create_if_not_exists: bool = True
    ) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        if not documents:
            log.warning("âš ï¸ æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ·»åŠ ")
            return
        
        total_docs = len(documents)
        log.info(f"ğŸ“ å‡†å¤‡æ·»åŠ  {total_docs} ä¸ªæ–‡æ¡£ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # å¦‚æœå‘é‡å­˜å‚¨ä¸å­˜åœ¨ä¸”å…è®¸åˆ›å»º
        if self._vector_store is None:
            if create_if_not_exists:
                first_batch = documents[:batch_size]
                log.info(f"ğŸ†• åˆ›å»ºæ–°å‘é‡å­˜å‚¨ï¼Œä½¿ç”¨å‰ {len(first_batch)} ä¸ªæ–‡æ¡£")
                self._vector_store = FAISS.from_documents(first_batch, self.embeddings)
                log.info(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼Œå·²æ·»åŠ ç¬¬ 1 æ‰¹ ({len(first_batch)} ä¸ªæ–‡æ¡£)")
                
                # åˆ†æ‰¹å¤„ç†å‰©ä½™æ–‡æ¡£
                remaining_docs = documents[batch_size:]
                if remaining_docs:
                    self._add_documents_in_batches(remaining_docs, batch_size, start_batch=2)
            else:
                raise ValueError("å‘é‡å­˜å‚¨ä¸å­˜åœ¨ä¸”ä¸å…è®¸åˆ›å»ºæ–°å­˜å‚¨")
        else:
            # å‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œåˆ†æ‰¹æ·»åŠ æ‰€æœ‰æ–‡æ¡£
            log.info(f"ğŸ“š å‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œå¼€å§‹åˆ†æ‰¹æ·»åŠ  {total_docs} ä¸ªæ–‡æ¡£")
            self._add_documents_in_batches(documents, batch_size, start_batch=1)
        
        log.info(f"ğŸ‰ æ‰€æœ‰æ–‡æ¡£æ·»åŠ å®Œæˆï¼æ€»è®¡ {total_docs} ä¸ªæ–‡æ¡£")
    
    def _add_documents_in_batches(
        self, 
        documents: List[Document], 
        batch_size: int, 
        start_batch: int = 1
    ) -> None:
        """åˆ†æ‰¹æ·»åŠ æ–‡æ¡£çš„å†…éƒ¨æ–¹æ³•"""
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_num = i // batch_size + start_batch
            log.info(f"â³ æ­£åœ¨å¤„ç†ç¬¬ {batch_num} æ‰¹ ({len(batch)} ä¸ªæ–‡æ¡£)")
            self._vector_store.add_documents(batch)
            log.info(f"âœ… ç¬¬ {batch_num} æ‰¹æ·»åŠ æˆåŠŸ")
    
    @traceable(name="similarity_search")
    def similarity_search(
        self, 
        query: str, 
        k: int = None, 
        filter: Optional[Dict[str, Any]] = None,
        fetch_k: int = 20
    ) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if self._vector_store is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£")
        
        k = k or settings.search_k
        return self._vector_store.similarity_search(
            query, k=k, filter=filter, fetch_k=fetch_k
        )
    
    @traceable(name="similarity_search_with_score")
    def similarity_search_with_score(
        self, 
        query: str, 
        k: int = None, 
        filter: Optional[Dict[str, Any]] = None
    ) -> List[tuple]:
        """å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢"""
        if self._vector_store is None:
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£")
        
        k = k or settings.search_k
        return self._vector_store.similarity_search_with_score(
            query, k=k, filter=filter
        )
    
    def save_vector_store(self, path: Optional[str] = None) -> None:
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜
        
        Args:
            path: ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è·¯å¾„
        """
        if self._vector_store is None:
            log.warning("âš ï¸ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return
        
        save_path = path or self.vector_store_path
        persist_dir = Path(save_path)
        
        try:
            persist_dir.mkdir(parents=True, exist_ok=True)
            self._vector_store.save_local(str(persist_dir), index_name=self.index_name)
            log.info(f"ğŸ’¾ å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {persist_dir}")
        except Exception as e:
            log.error(f"âŒ ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise e
    
    def load_vector_store(self, path: Optional[str] = None) -> None:
        """ä»ç£ç›˜åŠ è½½å‘é‡å­˜å‚¨
        
        Args:
            path: åŠ è½½è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„è·¯å¾„
        """
        load_path = path or self.vector_store_path
        persist_dir = Path(load_path)
        
        try:
            index_path = persist_dir / f"{self.index_name}.faiss"
            if not index_path.exists():
                raise FileNotFoundError(f"å‘é‡å­˜å‚¨æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
            
            self._vector_store = FAISS.load_local(
                str(persist_dir),
                self.embeddings,
                allow_dangerous_deserialization=self.allow_dangerous_deserialization,
                index_name=self.index_name
            )
            log.info(f"ğŸ“‚ å‘é‡å­˜å‚¨åŠ è½½æˆåŠŸ: {persist_dir}")
        except Exception as e:
            log.error(f"âŒ åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise e
    
    def clear_vector_store(self) -> bool:
        """æ¸…é™¤å‘é‡æ•°æ®åº“"""
        log.info("ğŸš€ å¼€å§‹æ‰§è¡Œå‘é‡æ•°æ®åº“æ¸…é™¤æ“ä½œ")
        
        try:
            # æ¸…é™¤å†…å­˜ä¸­çš„å‘é‡å­˜å‚¨
            vector_store_status = "å­˜åœ¨" if self._vector_store is not None else "ä¸å­˜åœ¨"
            log.info(f"ğŸ“Š æ¸…é™¤å‰å†…å­˜å‘é‡å­˜å‚¨çŠ¶æ€: {vector_store_status}")
            
            self._vector_store = None
            log.info("âœ… å†…å­˜ä¸­çš„å‘é‡å­˜å‚¨å·²æ¸…é™¤")
            
            # æ£€æŸ¥ç£ç›˜ä¸Šçš„å‘é‡æ•°æ®åº“æ–‡ä»¶
            log.info(f"ğŸ“ å‘é‡æ•°æ®åº“è·¯å¾„: {self.persist_dir}")
            log.info(f"ğŸ“Š ç›®å½•æ˜¯å¦å­˜åœ¨: {'æ˜¯' if self.persist_dir.exists() else 'å¦'}")
            
            if self.persist_dir.exists():
                try:
                    files_in_dir = list(self.persist_dir.iterdir())
                    log.info(f"ğŸ“‹ ç›®å½•ä¸­çš„æ–‡ä»¶: {[f.name for f in files_in_dir]}")
                except Exception as e:
                    log.warning(f"âš ï¸ æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {e}")
                
                shutil.rmtree(self.persist_dir)
                log.info(f"âœ… ç£ç›˜å‘é‡æ•°æ®åº“å·²åˆ é™¤: {self.persist_dir}")
            else:
                log.info("â„¹ï¸ å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
            
            # é‡æ–°åˆ›å»ºç©ºçš„ç›®å½•
            self.persist_dir.mkdir(parents=True, exist_ok=True)
            log.info(f"âœ… å·²é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“ç›®å½•: {self.persist_dir}")
            
            # éªŒè¯ç›®å½•åˆ›å»ºæˆåŠŸ
            if self.persist_dir.exists():
                log.info("âœ… éªŒè¯: å‘é‡æ•°æ®åº“ç›®å½•åˆ›å»ºæˆåŠŸ")
            else:
                log.error("âŒ éªŒè¯: å‘é‡æ•°æ®åº“ç›®å½•åˆ›å»ºå¤±è´¥")
                return False
            
            log.info("ğŸ‰ å‘é‡æ•°æ®åº“æ¸…é™¤å®Œæˆï¼")
            return True
            
        except Exception as e:
            log.error(f"âŒ æ¸…é™¤å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            log.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            log.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return False
    
    def get_vector_store_info(self) -> Dict[str, Any]:
        """è·å–å‘é‡å­˜å‚¨ä¿¡æ¯"""
        info = {
            "vector_store_exists": self._vector_store is not None,
            "persist_dir": str(self.persist_dir),
            "persist_dir_exists": self.persist_dir.exists(),
            "index_name": self.index_name,
            "embedding_model": getattr(self.embeddings, 'model', 'unknown'),
        }
        
        if self.persist_dir.exists():
            try:
                files = list(self.persist_dir.iterdir())
                info["files_in_directory"] = [f.name for f in files]
                info["index_file_exists"] = (self.persist_dir / f"{self.index_name}.faiss").exists()
                info["pkl_file_exists"] = (self.persist_dir / f"{self.index_name}.pkl").exists()
            except Exception as e:
                info["directory_access_error"] = str(e)
        
        return info
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥å‘é‡å­˜å‚¨æ˜¯å¦å°±ç»ª"""
        return self._vector_store is not None


if __name__ == "__main__":
    # æµ‹è¯•å‘½ä»¤ï¼Œæ ¹ç›®å½•è·¯å¾„è¿è¡Œï¼šuv run python -m src.knowledge_qa.vector_store
    """æµ‹è¯•å‘é‡å­˜å‚¨ç®¡ç†å™¨"""
    print("="*100)
    print("å¼€å§‹æµ‹è¯•å‘é‡å­˜å‚¨ç®¡ç†å™¨")
    print("="*100)
    
    from .file_parser import TextFileParser
    
    # åˆ›å»ºå‘é‡å­˜å‚¨ç®¡ç†å™¨
    vector_store = VectorStore()
    
    # æ˜¾ç¤ºåˆå§‹ä¿¡æ¯
    info = vector_store.get_vector_store_info()
    print(f"\nğŸ“Š å‘é‡å­˜å‚¨ä¿¡æ¯:")
    for key, value in info.items():
        print(f"   {key}: {value}")
    
    # æµ‹è¯•æ–‡æ¡£åŠ è½½å’Œåˆ†æ®µ
    print(f"\nğŸ“– åŠ è½½æµ‹è¯•æ–‡æ¡£...")
    text = TextFileParser.parse_file("examples/ä¸‰å›½æ¼”ä¹‰.txt")
    print(f"æ–‡æ¡£é•¿åº¦: {len(text)} å­—ç¬¦")
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap
    )
    documents = splitter.create_documents([text])
    print(f"æ–‡æ¡£åˆ†æ®µæ•°é‡: {len(documents)}")
    
    # æµ‹è¯•æ·»åŠ æ–‡æ¡£
    print(f"\nğŸ“ æµ‹è¯•æ·»åŠ æ–‡æ¡£...")
    test_docs = documents[:5]  # åªæµ‹è¯•å‰5ä¸ªæ–‡æ¡£
    vector_store.add_documents(test_docs, batch_size=3)
    
    # æµ‹è¯•ä¿å­˜
    print(f"\nğŸ’¾ æµ‹è¯•ä¿å­˜å‘é‡å­˜å‚¨...")
    vector_store.save_vector_store()
    
    # æµ‹è¯•æœç´¢
    print(f"\nğŸ” æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢...")
    query = "ä¸´æ±Ÿä»™"
    results = vector_store.similarity_search(query, k=2)
    print(f"æœç´¢å…³é”®è¯: {query}")
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(result.page_content[:200] + "...")
    
    # æµ‹è¯•å¸¦åˆ†æ•°æœç´¢
    print(f"\nğŸ“Š æµ‹è¯•å¸¦åˆ†æ•°æœç´¢...")
    results_with_score = vector_store.similarity_search_with_score(query, k=2)
    for i, (doc, score) in enumerate(results_with_score, 1):
        print(f"ç»“æœ {i} (åˆ†æ•°: {score:.4f}): {doc.page_content[:100]}...")
    
    print("\n" + "="*100)
    print("âœ… å‘é‡å­˜å‚¨ç®¡ç†å™¨æµ‹è¯•å®Œæˆ!")
    print("="*100)
