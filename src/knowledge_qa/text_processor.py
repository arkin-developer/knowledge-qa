"""æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""

from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from .config import settings
from .log_manager import log


class TextProcessor:
    """æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""

    def __init__(self, chunk_size: int = None, chunk_overlap: int = None) -> None:
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†æ®µå™¨"""
        self.chunk_size = chunk_size or settings.chunk_size
        self.chunk_overlap = chunk_overlap or settings.chunk_overlap
        
        # åˆ›å»ºåˆ†æ®µå™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )

    def split_text(self, text: str) -> List[Document]:
        """åˆ†æ®µæ–‡æœ¬"""
        if not text or not text.strip():
            log.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return []
        
        documents = self.splitter.create_documents([text])
        log.info(f"ğŸ“ æ–‡æœ¬åˆ†æ®µå®Œæˆï¼Œå…± {len(documents)} æ®µ")
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†æ®µæ–‡æ¡£åˆ—è¡¨"""
        if not documents:
            log.warning("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        split_docs = self.splitter.split_documents(documents)
        log.info(f"ğŸ“ æ–‡æ¡£åˆ†æ®µå®Œæˆï¼Œä» {len(documents)} ä¸ªæ–‡æ¡£åˆ†æ®µä¸º {len(split_docs)} æ®µ")
        return split_docs
    
    def get_splitter_info(self) -> dict:
        """è·å–åˆ†æ®µå™¨ä¿¡æ¯"""
        return {
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "separators": self.splitter._separators,
            "is_recursive": hasattr(self.splitter, '_separators')
        }


if __name__ == "__main__":
    """æµ‹è¯•æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""
    print("="*100)
    print("å¼€å§‹æµ‹è¯•æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨")
    print("="*100)
    
    from .file_parser import TextFileParser
    
    # æ­¥éª¤1: è§£ææ–‡ä»¶
    print("\nğŸ“– æ­¥éª¤1: è§£ææ–‡ä»¶")
    text = TextFileParser.parse_file("examples/ä¸‰å›½æ¼”ä¹‰.txt")
    print(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"æ–‡æœ¬é¢„è§ˆ: {text[:100]}...\n")
    
    # æ­¥éª¤2: åˆå§‹åŒ–åˆ†æ®µå™¨å¹¶åˆ†æ®µ
    print("ğŸ“ æ­¥éª¤2: åˆå§‹åŒ–åˆ†æ®µå™¨å¹¶åˆ†æ®µ")
    text_processor = TextProcessor()
    
    # æ˜¾ç¤ºåˆ†æ®µå™¨é…ç½®ä¿¡æ¯
    splitter_info = text_processor.get_splitter_info()
    print(f"åˆ†æ®µå™¨é…ç½®:")
    for key, value in splitter_info.items():
        print(f"   {key}: {value}")
    
    documents: List[Document] = text_processor.split_text(text)
    print(f"\næ–‡æ¡£å·²åˆ†æ®µï¼Œå…± {len(documents)} æ®µ")
    
    # æ˜¾ç¤ºå‰3æ®µé¢„è§ˆ
    print(f"\nğŸ“‹ å‰3æ®µé¢„è§ˆï¼š")
    for i, doc in enumerate(documents[:3], 1):
        print(f"\nç¬¬ {i} æ®µ:")
        print(doc.page_content[:200] + "...")
        print("-"*100)
    
    # æ­¥éª¤3: æµ‹è¯•æ–‡æ¡£åˆ†æ®µåŠŸèƒ½
    print("\nğŸ“š æ­¥éª¤3: æµ‹è¯•æ–‡æ¡£åˆ†æ®µåŠŸèƒ½")
    test_docs = documents[:5]  # å–å‰5ä¸ªæ–‡æ¡£è¿›è¡Œæµ‹è¯•
    split_docs = text_processor.split_documents(test_docs)
    print(f"åŸå§‹æ–‡æ¡£æ•°: {len(test_docs)}")
    print(f"åˆ†æ®µåæ–‡æ¡£æ•°: {len(split_docs)}")
    
    # æ­¥éª¤4: æµ‹è¯•ä¸åŒé…ç½®çš„åˆ†æ®µå™¨
    print("\nâš™ï¸ æ­¥éª¤4: æµ‹è¯•ä¸åŒé…ç½®çš„åˆ†æ®µå™¨")
    custom_processor = TextProcessor(chunk_size=200, chunk_overlap=50)
    custom_docs = custom_processor.split_text(text[:1000])  # åªæµ‹è¯•å‰1000å­—ç¬¦
    print(f"è‡ªå®šä¹‰é…ç½®åˆ†æ®µç»“æœ: {len(custom_docs)} æ®µ")
    print(f"è‡ªå®šä¹‰é…ç½®: chunk_size=200, chunk_overlap=50")
    
    print("\n" + "="*100)
    print("âœ… æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨æµ‹è¯•å®Œæˆ!")
    print("="*100)
