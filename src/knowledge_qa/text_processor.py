"""æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""

from typing import List
import re
from difflib import SequenceMatcher
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from .config import settings
from .log_manager import log


class TextProcessor:
    """æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""

    def __init__(self, chunk_size: int = None, chunk_overlap: int = None) -> None:
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†æ®µå™¨"""
        self.chunk_size = chunk_size or settings.chunk_size
        self.chunk_overlap = chunk_overlap or settings.chunk_overlap
        
        # åˆ›å»ºåˆ†æ®µå™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )

    def split_text(self, text: str) -> List[Document]:
        """åˆ†æ®µæ–‡æœ¬"""
        if not text or not text.strip():
            log.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return []
        
        documents = self.splitter.create_documents([text])
        log.info(f"ğŸ“ æ–‡æœ¬åˆ†æ®µå®Œæˆï¼Œå…± {len(documents)} æ®µ")
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†æ®µæ–‡æ¡£åˆ—è¡¨"""
        if not documents:
            log.warning("âš ï¸ è¾“å…¥æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        split_docs = self.splitter.split_documents(documents)
        log.info(f"ğŸ“ æ–‡æ¡£åˆ†æ®µå®Œæˆï¼Œä» {len(documents)} ä¸ªæ–‡æ¡£åˆ†æ®µä¸º {len(split_docs)} æ®µ")
        return split_docs
    
    def get_splitter_info(self) -> dict:
        """è·å–åˆ†æ®µå™¨ä¿¡æ¯"""
        return {
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
            "separators": self.splitter._separators,
            "is_recursive": hasattr(self.splitter, '_separators')
        }

    def long_text_novel_split(self, text: str) -> List[Document]:
        """é•¿æ–‡æœ¬å°è¯´åˆ†æ®µï¼ŒæŒ‰ç…§ç« èŠ‚è¿›è¡Œåˆ†æ®µï¼ˆå¢å¼ºç‰ˆï¼Œå¤„ç†é‡å¤æˆ–ç›¸ä¼¼æ ‡é¢˜ï¼‰"""
        if not text or not text.strip():
            log.warning("âš ï¸ è¾“å…¥æ–‡æœ¬ä¸ºç©º")
            return []

        import re

        # å®šä¹‰ç« èŠ‚åŒ¹é…æ¨¡å¼
        chapter_patterns = [
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ç« .*',  
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+å›.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+èŠ‚.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+é›†.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+éƒ¨.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+å·.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ç¯‡.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+å†Œ.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+æœ¬.*',
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+è¯.*',
        ]

        combined_pattern = '|'.join(chapter_patterns)

        # æŸ¥æ‰¾æ‰€æœ‰ç« èŠ‚ä½ç½®
        chapters = [(m.start(), m.group().strip()) for m in re.finditer(combined_pattern, text, re.MULTILINE)]

        if not chapters:
            log.warning("âš ï¸ æœªæ‰¾åˆ°ç« èŠ‚æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ®µ")
            return self.split_text(text)

        # å»é‡ç›¸ä¼¼æ ‡é¢˜ï¼Œé¿å…é‡å¤ç« èŠ‚æˆªæ–­æ­£æ–‡
        filtered_chapters = []
        for start_pos, chapter_title in chapters:
            if filtered_chapters:
                prev_title = filtered_chapters[-1][1]
                ratio = SequenceMatcher(None, prev_title, chapter_title).ratio()
                if ratio > 0.8:  # ç›¸ä¼¼åº¦è¶…è¿‡ 80% å°±è®¤ä¸ºé‡å¤
                    continue
            filtered_chapters.append((start_pos, chapter_title))

        # åˆ›å»ºæ–‡æ¡£åˆ—è¡¨
        documents = []
        for i, (start_pos, chapter_title) in enumerate(filtered_chapters):
            end_pos = filtered_chapters[i + 1][0] if i + 1 < len(filtered_chapters) else len(text)
            chapter_content = text[start_pos:end_pos].strip()
            if chapter_content:
                doc = Document(
                    page_content=chapter_content,
                    metadata={
                        "chapter_title": chapter_title,
                        "chapter_number": i + 1,
                        "source": "novel_split",
                        "content_type": "chapter"
                    }
                )
                documents.append(doc)

        log.info(f"ğŸ“š å°è¯´æŒ‰ç« èŠ‚åˆ†æ®µå®Œæˆï¼Œå…± {len(documents)} ä¸ªç« èŠ‚")
        return documents


if __name__ == "__main__":
    # æµ‹è¯•å‘½ä»¤ï¼Œæ ¹ç›®å½•è·¯å¾„è¿è¡Œï¼šuv run python -m src.knowledge_qa.text_processor
    from .file_parser import FileParser

    txt_path = "examples/å‡¡äººä¿®ä»™ä¼ test.txt"
    txt = FileParser.parse_txt_raw(txt_path)
    text_processor = TextProcessor()
    documents = text_processor.long_text_novel_split(txt)
    for doc in documents[:20]:
        print(doc.metadata)
        print(doc.page_content[:100], len(doc.page_content))
        print("-"*100)

    # """æµ‹è¯•æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨"""
    # print("="*100)
    # print("å¼€å§‹æµ‹è¯•æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨")
    # print("="*100)
    
    # from .file_parser import FileParser
    
    # # æ­¥éª¤1: è§£ææ–‡ä»¶
    # print("\nğŸ“– æ­¥éª¤1: è§£ææ–‡ä»¶")
    # text = FileParser.parse_file("examples/ä¸‰å›½æ¼”ä¹‰.txt")
    # print(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    # print(f"æ–‡æœ¬é¢„è§ˆ: {text[:100]}...\n")
    
    # # æ­¥éª¤2: åˆå§‹åŒ–åˆ†æ®µå™¨å¹¶åˆ†æ®µ
    # print("ğŸ“ æ­¥éª¤2: åˆå§‹åŒ–åˆ†æ®µå™¨å¹¶åˆ†æ®µ")
    # text_processor = TextProcessor()
    
    # # æ˜¾ç¤ºåˆ†æ®µå™¨é…ç½®ä¿¡æ¯
    # splitter_info = text_processor.get_splitter_info()
    # print(f"åˆ†æ®µå™¨é…ç½®:")
    # for key, value in splitter_info.items():
    #     print(f"   {key}: {value}")
    
    # documents: List[Document] = text_processor.split_text(text)
    # print(f"\næ–‡æ¡£å·²åˆ†æ®µï¼Œå…± {len(documents)} æ®µ")
    
    # # æ˜¾ç¤ºå‰3æ®µé¢„è§ˆ
    # print(f"\nğŸ“‹ å‰3æ®µé¢„è§ˆï¼š")
    # for i, doc in enumerate(documents[:3], 1):
    #     print(f"\nç¬¬ {i} æ®µ:")
    #     print(doc.page_content[:200] + "...")
    #     print("-"*100)
    
    # # æ­¥éª¤3: æµ‹è¯•æ–‡æ¡£åˆ†æ®µåŠŸèƒ½
    # print("\nğŸ“š æ­¥éª¤3: æµ‹è¯•æ–‡æ¡£åˆ†æ®µåŠŸèƒ½")
    # test_docs = documents[:5]  # å–å‰5ä¸ªæ–‡æ¡£è¿›è¡Œæµ‹è¯•
    # split_docs = text_processor.split_documents(test_docs)
    # print(f"åŸå§‹æ–‡æ¡£æ•°: {len(test_docs)}")
    # print(f"åˆ†æ®µåæ–‡æ¡£æ•°: {len(split_docs)}")
    
    # # æ­¥éª¤4: æµ‹è¯•ä¸åŒé…ç½®çš„åˆ†æ®µå™¨
    # print("\nâš™ï¸ æ­¥éª¤4: æµ‹è¯•ä¸åŒé…ç½®çš„åˆ†æ®µå™¨")
    # custom_processor = TextProcessor(chunk_size=200, chunk_overlap=50)
    # custom_docs = custom_processor.split_text(text[:1000])  # åªæµ‹è¯•å‰1000å­—ç¬¦
    # print(f"è‡ªå®šä¹‰é…ç½®åˆ†æ®µç»“æœ: {len(custom_docs)} æ®µ")
    # print(f"è‡ªå®šä¹‰é…ç½®: chunk_size=200, chunk_overlap=50")
    
    # print("\n" + "="*100)
    # print("âœ… æ–‡æœ¬åˆ†æ®µå¤„ç†å™¨æµ‹è¯•å®Œæˆ!")
    # print("="*100)
