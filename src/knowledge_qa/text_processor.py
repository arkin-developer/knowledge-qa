"""æ–‡æœ¬åˆ†æ®µå’Œå‘é‡åŒ–"""

from typing import Optional, List
from pathlib import Path
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langsmith import traceable

from .config import settings
from .log_manager import log


class TextProcessor:
    """æ–‡æœ¬åˆ†æ®µå’Œå‘é‡åŒ–å¤„ç†å™¨"""

    def __init__(self) -> None:
        # åˆ›å»ºåˆ†æ®µå™¨
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.chunk_size,
            chunk_overlap=settings.chunk_overlap
        )

        # åˆ›å»ºåµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.siliconcloud_api_key,
            openai_api_base=settings.siliconcloud_api_base,
            model=settings.embedding_model
        )

        # å‘é‡å­˜å‚¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.vector_store: Optional[FAISS] = None
        self._try_load_existing_vector_store()

    def _try_load_existing_vector_store(self) -> None:
        """å°è¯•åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨"""
        try:
            persist_dir = Path(settings.vector_store_path)
            index_path = persist_dir / "index.faiss"
            if index_path.exists():
                self.vector_store = FAISS.load_local(
                    str(persist_dir),
                    self.embeddings,
                    allow_dangerous_deserialization=True,
                    index_name="index")
                log.info(f"åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨: {persist_dir}")
            else:
                log.info("æœªæ‰¾åˆ°ç°æœ‰å‘é‡å­˜å‚¨ï¼Œå°†åœ¨é¦–æ¬¡æ·»åŠ æ–‡æ¡£æ—¶åˆ›å»º")
        except Exception as e:
            log.warning(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}ï¼Œå°†åœ¨é¦–æ¬¡æ·»åŠ æ–‡æ¡£æ—¶åˆ›å»ºæ–°çš„")

    def split_text(self, text: str) -> List[Document]:
        """åˆ†æ®µæ–‡æœ¬"""
        return self.splitter.create_documents([text])

    @traceable(name="add_documents")
    def add_documents(self, documents: List[Document], batch_size: int = 50) -> None:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨

        Args:
            documents: è¦æ·»åŠ çš„æ–‡æ¡£åˆ—è¡¨
            batch_size: é¦–æ¬¡åˆ›å»ºå‘é‡å­˜å‚¨æ—¶çš„æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤50
        """
        try:
            if not documents:
                log.warning("æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æ·»åŠ ")
                return

            total_docs = len(documents)
            log.info(f"å‡†å¤‡æ·»åŠ  {total_docs} ä¸ªæ–‡æ¡£")

            # å¦‚æœå‘é‡å­˜å‚¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€æ‰¹åˆ›å»º
            if self.vector_store is None:
                first_batch = documents[:batch_size]
                log.info(f"é¦–æ¬¡æ·»åŠ æ–‡æ¡£ï¼Œä½¿ç”¨å‰ {len(first_batch)} ä¸ªæ–‡æ¡£åˆ›å»ºå‘é‡å­˜å‚¨")
                self.vector_store = FAISS.from_documents(
                    first_batch, self.embeddings)
                log.info(f"âœ… å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼Œå·²æ·»åŠ ç¬¬ 1 æ‰¹ ({len(first_batch)} ä¸ªæ–‡æ¡£)")

                # å¤„ç†å‰©ä½™æ–‡æ¡£ - ä¸€æ¬¡æ€§å…¨éƒ¨æ·»åŠ 
                remaining_docs = documents[batch_size:]
                if remaining_docs:
                    log.info(f"æ­£åœ¨ä¸€æ¬¡æ€§æ·»åŠ å‰©ä½™ {len(remaining_docs)} ä¸ªæ–‡æ¡£...")
                    self.vector_store.add_documents(remaining_docs)
                    log.info(f"âœ… å‰©ä½™æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            else:
                # å‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œä¸€æ¬¡æ€§æ·»åŠ æ‰€æœ‰æ–‡æ¡£
                log.info(f"å‘é‡å­˜å‚¨å·²å­˜åœ¨ï¼Œæ­£åœ¨ä¸€æ¬¡æ€§æ·»åŠ  {total_docs} ä¸ªæ–‡æ¡£...")
                self.vector_store.add_documents(documents)
                log.info(f"âœ… æ‰€æœ‰æ–‡æ¡£æ·»åŠ æˆåŠŸ")

            log.info(f"ğŸ‰ æ‰€æœ‰æ–‡æ¡£æ·»åŠ å®Œæˆï¼æ€»è®¡ {total_docs} ä¸ªæ–‡æ¡£")

        except Exception as e:
            log.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise e

    def save_vector_store(self) -> None:
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        if self.vector_store is None:
            log.warning("å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œæ— éœ€ä¿å­˜")
            return

        try:
            persist_dir = Path(settings.vector_store_path)
            persist_dir.mkdir(parents=True, exist_ok=True)
            self.vector_store.save_local(
                str(persist_dir), index_name="index")
            log.info(f"å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {persist_dir}")
        except Exception as e:
            log.error(f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise e

    @traceable(name="similarity_search")
    def similarity_search(self, query: str, k: int = settings.search_k) -> List[Document]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if self.vector_store is None:
            log.error("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£")
            raise ValueError("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
        return self.vector_store.similarity_search(query, k, filter=None)


if __name__ == "__main__":
    # æµ‹è¯•å‘½ä»¤ï¼Œæ ¹ç›®å½•è·¯å¾„è¿è¡Œï¼šuv run python -m src.knowledge_qa.text_processor
    print("="*100)
    print("å¼€å§‹æµ‹è¯•æ–‡æœ¬å¤„ç†å’Œå‘é‡åŒ–")
    print("="*100)
    
    from .file_parser import TextFileParser
    
    # æ­¥éª¤1: è§£ææ–‡ä»¶
    print("\næ­¥éª¤1: è§£ææ–‡ä»¶")
    text = TextFileParser.parse_file("examples/ä¸‰å›½æ¼”ä¹‰.txt")
    print(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"æ–‡æœ¬é¢„è§ˆ: {text[:100]}...\n")
    
    # æ­¥éª¤2: åˆå§‹åŒ–å¤„ç†å™¨å¹¶åˆ†æ®µ
    print("æ­¥éª¤2: åˆå§‹åŒ–å¤„ç†å™¨å¹¶åˆ†æ®µ")
    text_processor = TextProcessor()
    documents: List[Document] = text_processor.split_text(text)
    print(f"æ–‡æ¡£å·²åˆ†æ®µï¼Œå…± {len(documents)} æ®µ")
    print(f"\nå‰3æ®µé¢„è§ˆï¼š")
    for i, doc in enumerate(documents[:3], 1):
        print(f"\nç¬¬ {i} æ®µ:")
        print(doc.page_content[:200] + "...")
        print("-"*100)
    
    # æ­¥éª¤3: æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨å°æ‰¹é‡æµ‹è¯•ï¼‰
    print("\næ­¥éª¤3: æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨")
    test_batch_size = 10  # å…ˆæµ‹è¯•10ä¸ªæ–‡æ¡£
    print(f"æ³¨æ„ï¼šä¸ºäº†å¿«é€Ÿæµ‹è¯•ï¼Œä»…ä½¿ç”¨å‰ {test_batch_size} æ®µæ–‡æ¡£")
    text_processor.add_documents(documents[:test_batch_size], batch_size=5)
    
    # æ­¥éª¤4: ä¿å­˜å‘é‡å­˜å‚¨
    print("\næ­¥éª¤4: ä¿å­˜å‘é‡å­˜å‚¨")
    text_processor.save_vector_store()
    
    # æ­¥éª¤5: æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    print("\næ­¥éª¤5: æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢")
    query = "ä¸´æ±Ÿä»™"
    print(f"æœç´¢å…³é”®è¯: {query}")
    results = text_processor.similarity_search(query, k=3)
    print(f"\næ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœï¼š")
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(result.page_content[:300] + "...")
        print("-"*100)
    
    print("\n" + "="*100)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("="*100)
